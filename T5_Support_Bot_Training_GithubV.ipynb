{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aeZj3DFFF8-K"
      },
      "outputs": [],
      "source": [
        "from IPython import get_ipython\n",
        "from IPython.display import display\n",
        "# %%\n",
        "!pip install -q transformers datasets scikit-learn pandas\n",
        "\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import torch\n",
        "from datasets import Dataset, DatasetDict\n",
        "\n",
        "# 1. Load dataset\n",
        "df = pd.read_csv(\"dataset-tickets-multi-lang-4-20k.csv\")\n",
        "\n",
        "# Fill potential missing values in 'answer' with an empty string\n",
        "df['answer'] = df['answer'].fillna('')\n",
        "\n",
        "# 2. Prepare input-output format\n",
        "def format_example(row):\n",
        "    input_text = f\"question: {row['subject']} context: {row['body']}\"\n",
        "    output_text = row['answer']\n",
        "    return {\"input_text\": input_text, \"output_text\": output_text}\n",
        "\n",
        "formatted_data = df.apply(format_example, axis=1, result_type=\"expand\")\n",
        "\n",
        "# 3. Convert to Hugging Face Dataset\n",
        "hf_dataset = Dataset.from_pandas(formatted_data)\n",
        "\n",
        "# 4. Train-test split\n",
        "split_dataset = hf_dataset.train_test_split(test_size=0.1, seed=42)\n",
        "\n",
        "# 5. Load tokenizer\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "\n",
        "# 6. Tokenize function\n",
        "def tokenize(example):\n",
        "    model_inputs = tokenizer(example[\"input_text\"], max_length=512, truncation=True, padding=\"max_length\")\n",
        "    labels = tokenizer(example[\"output_text\"], max_length=128, truncation=True, padding=\"max_length\")\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "tokenized_datasets = split_dataset.map(tokenize, batched=True)\n",
        "\n",
        "# 7. Load model\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "\n",
        "# 8. Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./t5-support-bot\",\n",
        "    # Changed evaluation_strategy to eval_strategy\n",
        "    eval_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=2,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=100,\n",
        "    load_best_model_at_end=True,\n",
        ")\n",
        "\n",
        "# 9. Data collator\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
        "\n",
        "# 10. Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "# 11. Train the model\n",
        "trainer.train()\n",
        "\n",
        "# 12. Save final model\n",
        "model.save_pretrained(\"./final-t5-support-bot\")\n",
        "tokenizer.save_pretrained(\"./final-t5-support-bot\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Define the directory where your model and tokenizer are saved\n",
        "model_directory = \"./final-t5-support-bot\"\n",
        "\n",
        "# Define the name for your zip file\n",
        "zip_filename = \"final-t5-support-bot.zip\"\n",
        "\n",
        "# Create a ZipFile object in write mode\n",
        "with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "    # Walk through the directory and add each file to the zip\n",
        "    for root, dirs, files in os.walk(model_directory):\n",
        "        for file in files:\n",
        "            # Create the full path to the file\n",
        "            file_path = os.path.join(root, file)\n",
        "            # Add the file to the zip archive, maintaining the directory structure\n",
        "            # The arcname is the path within the zip file\n",
        "            arcname = os.path.relpath(file_path, model_directory)\n",
        "            zipf.write(file_path, arcname)\n",
        "\n",
        "print(f\"Model and tokenizer saved to {zip_filename}\")"
      ],
      "metadata": {
        "id": "LNIp8LW3GFZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -o final-t5-support-bot.zip -d ./final-t5-support-bot"
      ],
      "metadata": {
        "id": "ONZhPLcCGKzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "import torch\n",
        "\n",
        "# Load model and tokenizer\n",
        "model_path = \"final-t5-support-bot\"  # ðŸ”§ removed './'\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
        "model.eval()\n",
        "\n",
        "# Generate response function\n",
        "def generate_response(question, context, max_length=128):\n",
        "    input_text = f\"question: {question} context: {context}\"\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", truncation=True, max_length=512).to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(input_ids, max_length=max_length, num_beams=4, early_stopping=True)\n",
        "\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Test example\n",
        "test_question = \"Platform crash\"\n",
        "test_context = \"The analytics platform stopped working unexpectedly and restarting the MacBook did not help.\"\n",
        "\n",
        "response = generate_response(test_question, test_context)\n",
        "print(\"ðŸ’¬ Generated Response:\\n\", response)"
      ],
      "metadata": {
        "id": "5cHXK2qAGOX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test input 2\n",
        "test_question = \"How can I reset my password?\"\n",
        "test_context = \"I forgot my login credentials and I can't access my account anymore. The reset password link doesn't work.\"\n",
        "\n",
        "response = generate_response(test_question, test_context)\n",
        "print(\"Generated Response:\\n\", response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWVpCHGo3lCR",
        "outputId": "7da4e032-befc-45b3-c142-c27708112d75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Response:\n",
            " I have forgotten my login credentials and can't access my account anymore. Please let me know a suitable time to call you at tel_num> to discuss this further.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Create dummy logs folder and trainer_state.json with sample data\n",
        "os.makedirs(\"./logs\", exist_ok=True)\n",
        "\n",
        "dummy_log = {\n",
        "    \"log_history\": [\n",
        "        {\"step\": 10, \"loss\": 2.5},\n",
        "        {\"step\": 20, \"loss\": 2.0, \"eval_loss\": 2.1},\n",
        "        {\"step\": 30, \"loss\": 1.7, \"eval_loss\": 1.8},\n",
        "        {\"step\": 40, \"loss\": 1.3, \"eval_loss\": 1.4},\n",
        "        {\"step\": 50, \"loss\": 1.1, \"eval_loss\": 1.2},\n",
        "    ]\n",
        "}\n",
        "\n",
        "with open(\"./logs/trainer_state.json\", \"w\") as f:\n",
        "    json.dump(dummy_log, f)\n",
        "\n",
        "# Step 2: Load the trainer_state.json\n",
        "with open(\"./logs/trainer_state.json\") as f:\n",
        "    trainer_state = json.load(f)\n",
        "\n",
        "log_history = trainer_state.get(\"log_history\", [])\n",
        "\n",
        "# Step 3: Extract step, train loss and eval loss\n",
        "steps = [entry[\"step\"] for entry in log_history if \"loss\" in entry]\n",
        "train_loss = [entry[\"loss\"] for entry in log_history if \"loss\" in entry]\n",
        "eval_loss = [entry.get(\"eval_loss\", None) for entry in log_history if \"loss\" in entry]\n",
        "\n",
        "# Step 4: Plot training and eval loss\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(steps, train_loss, label=\"Train Loss\", marker='o')\n",
        "plt.plot(steps, eval_loss, label=\"Eval Loss\", marker='x')\n",
        "plt.xlabel(\"Step\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training and Evaluation Loss\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZVvzNsZBGetG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}